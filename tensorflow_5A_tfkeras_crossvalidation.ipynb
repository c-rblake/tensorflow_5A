{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Life Expectancy with World Health Organization data \n",
    "Health Organization data \n",
    "Data curated from Kaggle covers 22 columns of data. Here we try to predict Life Expectancy given the other columns as features, which include data such as disease, alcohol, and mortality.\n",
    "Original data: The data covers 193 countries and has been collected from the WHO data repository website and the corresponding economic data was collected from the United Nation website. \n",
    "\n",
    "\n",
    "# Cross Validating and tuning with Tensorflow.Keras Neural Network  regressor in Sklearn's cross validation\n",
    "The data is then pipelined and used to fit a neural network. This is then refactored into cross-validating and hyperparameter tuning pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "path = r\"Life Expectancy Data.csv\"\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2938 entries, 0 to 2937\n",
      "Data columns (total 22 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   Country                          2938 non-null   object \n",
      " 1   Year                             2938 non-null   int64  \n",
      " 2   Status                           2938 non-null   object \n",
      " 3   Life expectancy                  2928 non-null   float64\n",
      " 4   Adult Mortality                  2928 non-null   float64\n",
      " 5   infant deaths                    2938 non-null   int64  \n",
      " 6   Alcohol                          2744 non-null   float64\n",
      " 7   percentage expenditure           2938 non-null   float64\n",
      " 8   Hepatitis B                      2385 non-null   float64\n",
      " 9   Measles                          2938 non-null   int64  \n",
      " 10   BMI                             2904 non-null   float64\n",
      " 11  under-five deaths                2938 non-null   int64  \n",
      " 12  Polio                            2919 non-null   float64\n",
      " 13  Total expenditure                2712 non-null   float64\n",
      " 14  Diphtheria                       2919 non-null   float64\n",
      " 15   HIV/AIDS                        2938 non-null   float64\n",
      " 16  GDP                              2490 non-null   float64\n",
      " 17  Population                       2286 non-null   float64\n",
      " 18   thinness  1-19 years            2904 non-null   float64\n",
      " 19   thinness 5-9 years              2904 non-null   float64\n",
      " 20  Income composition of resources  2771 non-null   float64\n",
      " 21  Schooling                        2775 non-null   float64\n",
      "dtypes: float64(16), int64(4), object(2)\n",
      "memory usage: 505.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing is necessary\n",
    "However, I will leave preprocessing out of this version as I am trying to integrate that into the 5_b project.<br>\n",
    "I believe there is value to be had in incorporating more of the model and data together while validating.<br>\n",
    "In this project, we will drop from about 2900 eligible rows down to 1649 rows. These will be added in the next 5_b project.\n",
    "\n",
    "## Generally about the data\n",
    "General Trends for Life expectancy, here will try to make an independent and identically distributed model. TimeSeries approach comes to mind, as improvements happen over time and place. These two variables are dropped in the IID approach.\n",
    "A time-series approach would also give rise to some easy to interpret graphs/plots.<br>\n",
    "The yearly data here comes down to just being quite close to itself. That is we almost bootstrapped a country for data <br> \n",
    "I decided to exclude Country and Year, these two variables are somewhat leaky for our independent data. It is also an extra step for predicting new data if we do not have an embedded strategy for 'Country' name. \n",
    "There could be general trends on year, that is technological advancements and life expectancy say for the year 1948 would be different for 2020, again these are then time trends, not independent observations.<vr>\n",
    "I also find GDP to be somewhat of a leaky variable, but GDP + Total Expenditure make an interesting feature. The absolute amount of money spent on health care. As a result, a suggested Feature is 'Absolute amount per capital on health care'. That is how many dollars spent per person<br>\n",
    "On investigation of the data: The 'Total Expenditure' is faulty. It has a description of % Amount but is the actual amount. The same as the suggested feature. Which makes a 'relative measure' missing and will be added instead.<br>\n",
    "Some of the column names are have starting or trailing whitespaces and those are also to be fixed. Things need to look neat.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1649 entries, 0 to 2937\n",
      "Data columns (total 20 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   Status                           1649 non-null   object \n",
      " 1   Life expectancy                  1649 non-null   float64\n",
      " 2   Adult Mortality                  1649 non-null   float64\n",
      " 3   infant deaths                    1649 non-null   int64  \n",
      " 4   Alcohol                          1649 non-null   float64\n",
      " 5   percentage expenditure           1649 non-null   float64\n",
      " 6   Hepatitis B                      1649 non-null   float64\n",
      " 7   Measles                          1649 non-null   int64  \n",
      " 8   BMI                              1649 non-null   float64\n",
      " 9   under-five deaths                1649 non-null   int64  \n",
      " 10  Polio                            1649 non-null   float64\n",
      " 11  Total expenditure                1649 non-null   float64\n",
      " 12  Diphtheria                       1649 non-null   float64\n",
      " 13  HIV/AIDS                         1649 non-null   float64\n",
      " 14  Population                       1649 non-null   float64\n",
      " 15  thinness  1-19 years             1649 non-null   float64\n",
      " 16  thinness 5-9 years               1649 non-null   float64\n",
      " 17  Income composition of resources  1649 non-null   float64\n",
      " 18  Schooling                        1649 non-null   float64\n",
      " 19  Relative Expenditure             1649 non-null   float64\n",
      "dtypes: float64(16), int64(3), object(1)\n",
      "memory usage: 270.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df.columns = df.columns.str.strip()\n",
    "df['Relative Expenditure'] = df['Total expenditure'] / df['GDP']\n",
    "df = df.drop(labels=['Country','Year','GDP'], axis=1)\n",
    "df = df.dropna()\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Categorical Variable, Easy fix.\n",
    "'Status' is a categorical variable with developing and developed. Suggested change to one-hot encoding and kept. get_dummies works well. Lambda will also work on the series because it is binary: 1,0 will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Developing    1407\n",
      "Developed      242\n",
      "Name: Status, dtype: int64\n",
      "1    1407\n",
      "0     242\n",
      "Name: Status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['Status'].value_counts())\n",
    "df['Status'] = df['Status'].apply(lambda x: 1 if x == 'Developing' else 0)\n",
    "print(df['Status'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = df.pop('Life expectancy')\n",
    "features = df\n",
    "features.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split Approach\n",
    "We will also revisit Cross Validation which is prefered. <br>\n",
    "Tensorflow has internal validation and that is used later on in the cross validation. <br>\n",
    "There isn't anything in particular to stratify the splits on other than 'Status' there are enough points though the model would capture the distinctions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal validation possible in TF with validation_split. Splitting only for test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, train_size=0.8) # stratify on developing or other variables reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = features.select_dtypes(['float64','int64']) # or list comprehension + membership check [col for col in df.columns if df.columns[col] in ['float64','int64']]\n",
    "numeric_columns = numeric_features.columns\n",
    "ct = ColumnTransformer([(\"Numeric Scaler\", StandardScaler(), numeric_columns)], remainder='passthrough')\n",
    "fulldata_ct = ct\n",
    "# Zipping the standardscaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = ct.fit_transform(X_train) # This is similar to a function that would take in the data, calculate the mean and std for each column. Store that information and apply it to new data to z-standardize.\n",
    "X_test_scaled = ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing done. Note that nothing in the data was imputed and columns were dropped. If we suspect a Time Series is a good idea, that approach would also yield good results in my estimation. Especially visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Dense, Dropout\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = Sequential(name='my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_layer = InputLayer(input_shape=(features.shape[1],), name='input_layer') # (Feature columns, any number of rows.)\n",
    "my_model.add(input_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19 features and a close enough binary number. 2,4,8,16,32\n",
    "dense_1 = Dense(32, activation='relu', name='hidden_layer_one')\n",
    "#dense_2 = Dense(16, activation='relu', name='hidden_layer_two')\n",
    "my_model.add(dense_1)\n",
    "#my_model.add(dense_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = Dense(1, name='regression_output') # Regression, no activation function on this estimating layer\n",
    "my_model.add(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden_layer_one (Dense)     (None, 32)                640       \n",
      "_________________________________________________________________\n",
      "regression_output (Dense)    (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 673\n",
      "Trainable params: 673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(my_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the pieces for backpropagation in\n",
    "my_model.compile(loss='mse',metrics=['mae'], optimizer='Adam') # Standard Learning Rate for Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = my_model.fit(X_train_scaled, y_train, epochs=40, batch_size=1, verbose=False) # from ca 10 -> 2 MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330/330 [==============================] - 0s 205us/sample - loss: 9.5398 - mae: 2.1455\n"
     ]
    }
   ],
   "source": [
    "res_mse, res_mae = my_model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 9.54\n",
      "Mean Absolute Error: 2.15\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean Squared Error: {np.round(res_mse, decimals=2)}\")\n",
    "print(f\"Mean Absolute Error:\", np.round(res_mae, decimals=2)) # Notebook Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----FIVE ACTUAL VALUES ---\n",
      "2156    62.8\n",
      "2557    68.1\n",
      "1335    73.4\n",
      "1193    65.5\n",
      "1987    59.6\n",
      "Name: Life expectancy, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('-----FIVE ACTUAL VALUES ---')\n",
    "print(y_test.iloc[5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----FIVE PREDICTIONS VALUES ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[63.2666  ],\n",
       "       [69.29033 ],\n",
       "       [74.30104 ],\n",
       "       [64.472466],\n",
       "       [62.19476 ]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction = my_model.predict(np.array([X_test_scaled[1,:],])) Single prediction A Bit harder To give the model [[]]\n",
    "prediction = my_model.predict(np.array(X_test_scaled[5:10,:]))\n",
    "print('-----FIVE PREDICTIONS VALUES ---')\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.8"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(target.std(), decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So far the model does pretty well.\n",
    "We have an absolute Error of about 2.27 years which is quite low. The Standard deviation is roughly 8.8 and the variance of  77 for the target data. So the model does a lot better at about, at Mean absolute error of 2.28 and Mean Squared Error 9.69. MAE is computed a bit different from standard deviation but mostly are in the same ballpark. \n",
    "It is somewhat like an R-squared value for explanatory value except this one is in the smaller_is_better schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use all the data?\n",
    "Technically speaking, there is not much need for such a large test set if we are doing cross-validation. \n",
    "And so we could use our CT object on more of the data set at this point.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target, train_size=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = fulldata_ct.fit_transform(X_train) # In case we want to do more transforms the object is now saved.\n",
    "X_test = fulldata_ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One more time With a Random Search\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "from scipy.stats import randint as sp_randint\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def design_model():\n",
    "    model = Sequential(name=\"my_model\")\n",
    "    input = tf.keras.Input(shape=(features.shape[1],)) # Cannot use input_shape with Cross Validation\n",
    "    model.add(input)\n",
    "    model.add(Dense(11, activation = 'relu'))\n",
    "    model.add(Dense(1))\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = 0.01)\n",
    "    model.compile(loss='mse', metrics=['mae'], optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_randomized_search(design=design_model):\n",
    "  param_grid = {'batch_size': sp_randint(2, 16), 'nb_epoch': sp_randint(10, 100)}\n",
    "  model = KerasRegressor(build_fn=design)\n",
    "  grid = RandomizedSearchCV(estimator = model, param_distributions=param_grid, \n",
    "                            scoring = make_scorer(mean_squared_error, greater_is_better=False), \n",
    "                            n_iter = 12, n_jobs=-1) # njobs -1 uses all processors if one has more to spare.\n",
    "  es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience = 20)\n",
    "  random_search = grid.fit(X_train, y_train, verbose = 0, callbacks=[es]) # es goes here?\n",
    " \n",
    "  return random_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- RANDOMIZED SEARCH COMPLETED--------------------\n"
     ]
    }
   ],
   "source": [
    "random_search = do_randomized_search()\n",
    "print(\"-------------- RANDOMIZED SEARCH COMPLETED--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean Score</th>\n",
       "      <th>Standard Dev</th>\n",
       "      <th>Parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-114.186734</td>\n",
       "      <td>17.984845</td>\n",
       "      <td>{'batch_size': 3, 'nb_epoch': 55}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-161.383234</td>\n",
       "      <td>26.037532</td>\n",
       "      <td>{'batch_size': 4, 'nb_epoch': 28}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-184.535867</td>\n",
       "      <td>31.999850</td>\n",
       "      <td>{'batch_size': 4, 'nb_epoch': 52}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-265.861618</td>\n",
       "      <td>31.616364</td>\n",
       "      <td>{'batch_size': 6, 'nb_epoch': 28}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-518.913632</td>\n",
       "      <td>186.794057</td>\n",
       "      <td>{'batch_size': 8, 'nb_epoch': 66}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1222.874248</td>\n",
       "      <td>295.127890</td>\n",
       "      <td>{'batch_size': 12, 'nb_epoch': 62}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1265.548907</td>\n",
       "      <td>222.803490</td>\n",
       "      <td>{'batch_size': 11, 'nb_epoch': 21}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1386.279310</td>\n",
       "      <td>210.828040</td>\n",
       "      <td>{'batch_size': 13, 'nb_epoch': 93}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1571.947558</td>\n",
       "      <td>187.049547</td>\n",
       "      <td>{'batch_size': 12, 'nb_epoch': 54}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1818.998144</td>\n",
       "      <td>627.104025</td>\n",
       "      <td>{'batch_size': 13, 'nb_epoch': 35}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Mean Score  Standard Dev                          Parameters\n",
       "9   -114.186734     17.984845   {'batch_size': 3, 'nb_epoch': 55}\n",
       "0   -161.383234     26.037532   {'batch_size': 4, 'nb_epoch': 28}\n",
       "5   -184.535867     31.999850   {'batch_size': 4, 'nb_epoch': 52}\n",
       "10  -265.861618     31.616364   {'batch_size': 6, 'nb_epoch': 28}\n",
       "1   -518.913632    186.794057   {'batch_size': 8, 'nb_epoch': 66}\n",
       "8  -1222.874248    295.127890  {'batch_size': 12, 'nb_epoch': 62}\n",
       "4  -1265.548907    222.803490  {'batch_size': 11, 'nb_epoch': 21}\n",
       "2  -1386.279310    210.828040  {'batch_size': 13, 'nb_epoch': 93}\n",
       "6  -1571.947558    187.049547  {'batch_size': 12, 'nb_epoch': 54}\n",
       "7  -1818.998144    627.104025  {'batch_size': 13, 'nb_epoch': 35}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random = pd.DataFrame({'Mean Score':random_search.cv_results_['mean_test_score'],\n",
    "                      'Standard Dev': random_search.cv_results_['std_test_score'],\n",
    "                       'Parameters': random_search.cv_results_['params']\n",
    "                      })\n",
    "# cv_results_:dict of numpy (masked) ndarrays\n",
    "random.sort_values(by='Mean Score', ascending=False, inplace=True)\n",
    "random.head(5).append(random.tail(5)) # Appends to the end of df, Best and worst performers.\n",
    "# we see a Tendency for small Batch Sizes and  > 43 epochs\n",
    "# We could use this information to sort of do Bayesian estimations ourselvess. \n",
    "# Clearly smaller batches are better at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/sample - loss: 88.4128 - mae: 7.3081\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "88.41277657933982"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_estimator = random_search.best_estimator_\n",
    "# TF res_mse, res_mae = best_estimator.evaluate(X_test_scaled, y_test)\n",
    "-1* best_estimator.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Cross Validated model performs worse. Why?\n",
    "Notice the difference. The instantiated model from the designmodel function has a smaller number of nodes.\n",
    "11 Nodes was used here since I took the code of another dataset in the tensorflow_4 project. \n",
    "In this project, we were using binary stacks of nodes. I did a manual increase of hidden layers without much success, however, reducing the model's number of nodes makes it worse off.\n",
    "However, we do notice that smaller batches and high epochs seem to work well for the model. <br>\n",
    "In the 32 nodes, we had a loss of about 9 and 2 years. The 11 node seems to do about 80MSE loss and 7 years off.\n",
    "The lower complexity model does not capture enough of the data. So we can say that at 11 -> 32 there is computational reducibility of the problem. We can bridge this reducibility by increasing the complexity of the model, sometimes this is also referred to as a bias-variance trade-off. Tweaking hyperparameters does not make up for it at this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to refactor the Code to be able to do more exhaustive searches\n",
    "The continuation on this project will be work on refactoring the code to fit to two of my ideas on computational reducibility and also taking into account Bayesian optimization thinking, the last two DataFrames shows a good (indicaiton on how to optimize two hyperparameters) and iterative modeling. \n",
    "The code here is enough of a start for those two refactornigs.\n",
    "My thinking is that I want the code to first add complexity then add optimization, and keep going like this. Added on to the 5_b project will be the viking model that did just those things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What else can be done? Get the Data back\n",
    "Another important piece is all that data that was dropped. Since it had year and country columns, I am pretty sure the first draft of TimeSeries interpolation could help bridge those gaps. \n",
    "I would not at this stage want to impute the values. After using the interpolation approach, we could do an iterative play between imputation and interpolation. <br>\n",
    "That is the best estimation for the country's missing values is the closest existing value. If this doesn't exist then the closest neighbor by Vectorizing each country and year could be used. I would use the Cosine similarity often used in natural language processing to figure it out. So those are two ways to improve on the data.<br>\n",
    "I will end up trying to impute with a KNNimputer which is close to cosine similarity. <br>\n",
    "Barring that, we could use a stratified split to infer values and finally, impute the thing with the mean for the column. However imputing with the mean is well, strange. The missing values will tend to come from poorer nations in general and these will have different life expectancies from the mean. Since it is almost half the data I would hesitate to use the mean imputation for a first go. \n",
    "### SPOILER: taking into account outliers makes for more trouble."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
